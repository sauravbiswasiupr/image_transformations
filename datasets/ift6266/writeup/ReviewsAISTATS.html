<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0096)https://cmt.research.microsoft.com/AIS2011/Protected/Author/ViewReviewsForPaper.aspx?paperId=126 -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>
	Reviews For Paper
</title>
<style>
#header
{
    width: 100%;
    font-size: small;
    background-color:#F7F7F7;
}
.printThemeText
{
    font-size:small;
}
.printThemeTable td
{
    vertical-align:top;
}
.printThemeGrid th
{
    color:white;
    background:#5D7B9D;
    font-weight:bold;
}
.printThemeGrid
{
    border-collapse:collapse;
}
.printThemeGrid td, .printThemeGrid th
{
    border:solid 1px #D6D3CE;
    padding:4px 4px 4px 4px;
}
.printThemeGrid .row
{ 
    background-color:#F7F6F3;
    color:#333333;
    vertical-align:top;
}
.printThemeGrid .altrow
{ 
    background-color:White;
    color:#284775;
    vertical-align:top;
}
.cellprompt
{
	font-weight:bold;
	white-space:nowrap;
    width:100px;	
}
.paperHeader
{
    background-color:#dee3e7;
    margin:5px 5px 15px 0px;
    width:99%;
    font-family:Verdana;
    font-size:medium;
    font-weight:bold;
}
.sectionHeader
{
    background-color:#dee3e7;
    padding:5px 5px 5px 0px;
    width:99%;
    text-decoration:underline;
    font-family:Verdana;
    font-size:small;
    font-weight:bold;
}
.underlineheader
{
    text-decoration:underline;
    font-weight:bold;
    padding:5px 0px;
}
.response
{
    padding:5px 0px;
}
.reviewerlabel
{
    padding-right:20px;
}
.pageTitle
{
    background-color:#dee3e7;
    padding:5px 5px 5px 5px;
    margin-top:10px;
    width:99%;
    font-family:Verdana;
    font-size:medium;
    font-weight:bold;
}
.submissionDetailsView
{
}
.submissionDetailsView tr
{
    vertical-align:top;
}
.submissionDetailsView td.prompt
{
    font-weight:bold;
}
.submissionDetailsView tr.sectionSeparator
{

}
.submissionDetailsView tr.sectionSeparator td
{
    background-color:#dee3e7;
    padding:5px 5px 5px 5px;
    font-family:Verdana;
    font-size:small;
    font-weight:bold;
    color:Navy;
}
</style>
</head>
<body>
<form name="aspnetForm" method="post" action="./ReviewsAISTATS_files/ReviewsAISTATS.html" id="aspnetForm">
<div>
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="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">
</div>

<table id="header">
<tbody><tr>
<td><img src="./ReviewsAISTATS_files/conferencelogo.gif"></td>
<td width="100%"><a href="http://www.aistats.org/">AI &amp; Statistics 2011 </a><br><b>Fourteenth International Conference on Artificial Intelligence and Statistics </b><br>April 11-13, 2011<br>Ft. Lauderdale, FL<br>USA</td>
</tr>
</tbody></table>
<table id="content"><tbody><tr><td class="contentBorder">&nbsp;</td><td class="contentContainer">
<span id="ctl00_cph_Label4" style="font-size:Small;font-weight:bold;">Reviews For Paper</span>
<span id="ctl00_cph_lblErrorMessage" class="error" style="font-size:Small;"></span>
<div id="ctl00_cph_pnlReviews">
	
    <span style="font-size:Small;">
<table class="nicetable2" style="text-align:left; width: 100%;">
    
    <tbody><tr>
        <td width="100px"><b>Paper ID</b></td>
        <td><span id="ctl00_cph_infoSubmission_lblPaperId" style="font-size:Small;">126</span></td>
    </tr>
    <tr>
        <td><b>Title</b></td>
        <td><span id="ctl00_cph_infoSubmission_lblPaperTitle" style="font-size:Small;">Deep Learners Benefit More from Out-of-Distribution Examples</span></td>
    </tr>
    
    
    
    
    
</tbody></table></span>
    
    
            <hr>
            <table>
                <tbody><tr>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl00_Label2" style="font-size:Small;font-weight:bold;">Masked Reviewer ID:</span>
                    </td>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl00_Label1" style="font-size:Small;">Assigned_Reviewer_2</span>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl00_Label3" style="font-size:Small;font-weight:bold;">Review:</span>
                    </td>
                    <td>
                    </td>
                </tr>
            </tbody></table>
            <div>
		<table cellspacing="0" cellpadding="4" rules="all" border="1" style="color:#333333;border-width:1px;border-style:None;font-family:Verdana;font-size:Small;border-collapse:collapse;">
			<tbody><tr style="color:White;background-color:#5D7B9D;font-weight:bold;">
				<th scope="col">Question</th><th scope="col">&nbsp;</th>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Overall rating: please synthesize your answers to other questions into an overall recommendation.  Please take into account tradeoffs (an increase in one measure may compensate for a decrease in another), and describe the tradeoffs in the detailed comments.</td><td style="width:80%;">
                            Good: suggest accept
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Technical quality: is all included material presented clearly and correctly?</td><td style="width:80%;">
                            Good
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Originality: how much new work is represented in this paper, beyond previous conference/journal papers?</td><td style="width:80%;">
                            Substantial new material
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Interest and significance: would the paper's goal, if completely solved, represent a substantial advance for the AISTATS community?</td><td style="width:80%;">
                            Significant
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Thoroughness: to what degree does the paper support its conclusions through experimental comparisons, theorems, etc.?</td><td style="width:80%;">
                            Thorough
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Creativity: to what degree does the paper represent a novel way of setting up a problem or an unusual approach to solving it?</td><td style="width:80%;">
                            Most content represents application of known ideas
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Detailed Comments</td><td style="width:80%;">
                            This paper shows that deep networks benefit more from out-of-distribution examples than shallower architectures on a large scale character recognition experiment. A thorough empirical validation shows that deep nets produce better discrimination (than shallower nets) when trained with distorted characters and when trained on multiple tasks. 
<br>Although the methods used are already well established in the community, these results are significant and provide new insights on the representational power of this class of methods.
<br>
<br>Suggestions:
<br>- it would be interesting to compare the deep architecture and the shallow architecture for a given capacity of the model (i.e. use wider shallow net)
<br>- since the authors use denoising autoencoders to pre-train deep networks, they could consider to use distorted characters as noisy inputs instead of artificially set to 0 some inputs. This might help learning more representations that are more robust to distortions that are actually useful for discrimination.
                        </td>
			</tr>
		</tbody></table>
	</div>
            
        
            <hr>
            <table>
                <tbody><tr>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl01_Label2" style="font-size:Small;font-weight:bold;">Masked Reviewer ID:</span>
                    </td>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl01_Label1" style="font-size:Small;">Assigned_Reviewer_3</span>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span id="ctl00_cph_gvReviews_ctl01_Label3" style="font-size:Small;font-weight:bold;">Review:</span>
                    </td>
                    <td>
                    </td>
                </tr>
            </tbody></table>
            <div>
		<table cellspacing="0" cellpadding="4" rules="all" border="1" style="color:#333333;border-width:1px;border-style:None;font-family:Verdana;font-size:Small;border-collapse:collapse;">
			<tbody><tr style="color:White;background-color:#5D7B9D;font-weight:bold;">
				<th scope="col">Question</th><th scope="col">&nbsp;</th>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Overall rating: please synthesize your answers to other questions into an overall recommendation.  Please take into account tradeoffs (an increase in one measure may compensate for a decrease in another), and describe the tradeoffs in the detailed comments.</td><td style="width:80%;">
                            Very good: suggest accept
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Technical quality: is all included material presented clearly and correctly?</td><td style="width:80%;">
                            Very good
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Originality: how much new work is represented in this paper, beyond previous conference/journal papers?</td><td style="width:80%;">
                            Substantial new material
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Interest and significance: would the paper's goal, if completely solved, represent a substantial advance for the AISTATS community?</td><td style="width:80%;">
                            Significant
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Thoroughness: to what degree does the paper support its conclusions through experimental comparisons, theorems, etc.?</td><td style="width:80%;">
                            Thorough
                        </td>
			</tr><tr style="color:#284775;background-color:White;">
				<td style="width:20%;">Creativity: to what degree does the paper represent a novel way of setting up a problem or an unusual approach to solving it?</td><td style="width:80%;">
                            Most content represents novel approaches
                        </td>
			</tr><tr style="color:#333333;background-color:#F7F6F3;">
				<td style="width:20%;">Detailed Comments</td><td style="width:80%;">
                            This paper claims that using out-of-distribution examples can be more helpful in training deep architectures than shallow architectures. In order to test this hypothesis, the paper develops extensive transformations for image patches (i.e., images of handwritten characters) to generate a large-scale dataset of perturbed images. These out-of-distribution examples are trained using MLPs and stacked denoising auto-encoders (SDAs). In the experiments, the paper shows that SDAs outperform MLPs, achieving human-level performance for NIST dataset. The paper also provides two interesting experiments showing that: (1) SDAs can benefit from training perturbed data, even when testing on clean data; (2) SDAs can significantly benefit from multi-task learning.
<br>
<br>
<br>Questions, comments, and suggestions:
<br>1. Regarding the human labeling, I have some concerns about labeling noise/biases due to AMT. How were the anomalies in labeling or outliers controlled? Was there any procedure to minimize labeling noise/biases or to ensure that human labelers tried their best (e.g., filtering out random guesses or encouraging the labelers to consider all possibilities carefully before providing premature guesses)? For example, multi-stage questionnaires (e.g., asking "characters/digits", "uppercase/lowercase", then choosing one out of 10 digits, or 26 characters) might significantly reduce labeling noise/biases, rather than showing 62 candidate answers simultaneously.
<br>
<br>2. It seems that the paper fixed the number of hidden layers as three. Despite good performance of the proposed architecture, it is somewhat unclear whether the benefit comes mainly from deep architecture or the use of denoising auto-encoders.
<br>
<br>Therefore, it will be more interesting to see the effect of the number of layers and other pre-training methods (e.g., RBMs or auto-encoders). This experiment will clarify where the benefit comes from (i.e., deep architecture vs. pre-training modules) and provide more insights about the results.
<br>
<br>3. The paper briefly mentioned about the use of libSVM, but it will be useful to compare against the results using online SVM (e.g., PEGASOS).
<br>
<br>4. The paper also talks about the effect of large labeled data in self-taught learning setting. To strengthen the claim, it will be helpful to show the test accuracy as a function of number of labeled examples.
<br>
<br>Overall, the paper is clearly written, and it provides interesting experiments on large scale datasets, addressing a number of interesting questions related to deep learning and multi-task learning. Furthermore, this work can provide a new large scale benchmark dataset (beyond MNIST) for deep learning and machine learning research.
<br>
                        </td>
			</tr>
		</tbody></table>
	</div>
            
        
    <br>
    <br>

</div>
</td><td class="contentBorder">&nbsp;</td></tr></tbody></table>
</form>


</body></html>